---
title: "D208 - Predictive Modeling"
output:
  html_notebook:
    toc: yes
    theme: spacelab
  html_document:
    toc: yes
    df_print: paged
  word_document:
    toc: yes
  pdf_document:
    toc: yes
---

</br>

------------------------------------------------------------------------

#### **Performance Assessment - Task 2: Logistic Regression for Predictive Modeling**

#### *Medical Readmission Data Set (Clean)*

------------------------------------------------------------------------

</br>

```{r, include=FALSE}
# Pre-execute necessary code
suppressMessages(library(tidyverse))
suppressMessages(library(skimr))
suppressMessages(library(Hmisc))
suppressMessages(library(broom))
suppressMessages(library(car))
suppressMessages(library(MASS))
suppressMessages(library(InformationValue))

df <- read.csv("./data/medical_clean.csv")

init_mdl <- df %>%
  dplyr::select(Initial_days,
         Area,
         Children:Services,
         Additional_charges)

# Reformat all chr variables as fct
init_mdl[sapply(init_mdl,
                is.character)] <- lapply(init_mdl[sapply(init_mdl,
                                                         is.character)],
                                         as.factor)

# Initial/upper logit model
logitfull <- glm(ReAdmis ~ .,
                data = init_mdl,
                family = binomial)

# Lower logit model
logitlow <- glm(ReAdmis ~ 1,
                data = init_mdl,
                family = binomial)

# Initiate stepfwd model
stepfwd <- suppressWarnings(stepAIC(logitlow,
                                    scope = list(lower = logitlow,
                                                 upper = logitfull)))

# Initiate stepbck model
stepbck <- suppressWarnings(stepAIC(logitfull,
                                    direction = "backward"))
```

## Part I
___
### A1: Research Question

The central research question addressed by this analysis is to determine:

>What variables from the medical dataset influence a patient's probability of being readmitted to the hospital following an initial admission (`ReAdmis`)? 

In terms of hypothesis testing, our null hypothesis ($H_0$) is:

>One or more variables included in the medical dataset influence readmission probability (`ReAdmis`) in any statistically significant way.

Additionally, our alternate hypothesis ($H_1$) is:

>No variables included in the medical dataset influence readmission probability (`ReAdmis`) in a statistically significant way.

</br>

### A2: Objectives and Goals

The primary goal of the following analysis is to discover what predictors potentially increase the likelihood a patient will be readmitted after initial hospitalization (`ReAdmis`). This will be assessed using the $\mbox{R}$ programming language using the technique of multiple logistic regression to identify causal relationships between one or more predictor variables and a binary target variable.

___
</br>
</br>

## Part II
___
### B1: Summary of Assumptions

The following are basic assumptions of logistic regression:

-   The assumption of binary predictor outcome (between 0 and 1)
-   The assumption of linearity of continuous predictor variables
-   The assumption of non-multicollinearity of predictor variables
-   The assumption of no significantly influential outliers in dataset

</br>


### B2: Tool Benefits

The programming language of choice for this analysis, as previously mentioned, will be the $\mbox{R}$ programming language. Previously, this author has used Python to perform cleaning, transformation, and analysis. Python has been more than up to the task. However, $\mbox{R}$ happens to handle the process of regression analysis and model selection exceptionally well and thus became the self-evident choice for multiple logistic regression analysis. In particular, the built-in functions of the base $\mbox{R}$ language, as used to fit models, are incredibly simple to navigate and equally as easy to demonstrate. Additionally, the reduction process using the `stepAIC()` function, as another example, made the task of choosing which programming language to use for this project even easier. It is with good reason that $\mbox{R}$ has such a stellar reputation for handling regression models.

</br>

### B3: Appropriate Technique

Multiple logistic regression is an appropriate technique to use to accomplish our goal of finding which variables contribute to a potentially higher probability of readmission for a given patient (`ReAdmis`) for several reasons. Firstly, the dataset we will analyze contains 50 variables in total, each of disparate significance and utility. While some variables do not require much thought prior to elimination, others are not quite as straightforward. Therefore, running them through a multiple logistic regression model prior to discarding them is an appropriate course of action. Additionally, multiple regression will allow us to see the significance of each variable's discrete contribution to the target variable as well as the interaction between explanatory variables themselves. Overall, regarding our objective, multiple logistic regression is more than up to the task and will adequately suit our objectives.

___
</br>
</br>

## Part III

___

### C1: Data Goals

The process we will need to complete in order to prepare the data for model selection is relatively minor, given that the raw dataset used in this project has already been cleaned in a prior project (see project D206 - Data Cleaning). Using the pre-cleaned dataset, we will first partition the data to include only those variables we intend to feed into our initial model. Because the first model selection process we will use is backward-oriented, this initial model will include all features that could possibly have a relationship to the binary target variable of readmission (`ReAdmis`). This will include a mix of numeric and categorical variables. 

Next, we will need to ensure that the data type of each variable is appropriate for that kind of feature. For example, we will determine which variables are categorical and need to be coded as a factor in $\mbox{R}$. Once the dataset for the initial model has been partitioned and transformed (or converted to the right type, at least), we will look over the dataset to ensure that we have not created any problems in the process such as silently introducing null values.

</br>

### C2: Summary Statistics

In order to get the best understanding of the selected features and their measures of central tendency, we will use an amazing library called `skimr` which does a phenomenal job of not only providing a great default summary view of the entire dataframe, but also allows one to customize the output. First, we will fashion a version of the skim function purpose-built for our needs here and print the output.

```{r}
# Set custom skim() for C2: Summary Statistics
# For numeric include mean, median, stdev, min, Q25, Q75, and max
# For factor include count of unique values and value counts for each
my_skim <- skim_with(
  base = sfl(),
  numeric = sfl(Mean = mean,
                Median = median,
                StDev = sd,
                Min = min,
                Q25 = ~ quantile(., probs = .25),
                Q75 = ~ quantile(., probs = .75),
                Max = max),
  factor = sfl(Unique_Values = n_unique,
               Value_Counts = top_counts),
  append = FALSE
)

# Call new skim format
my_skim(init_mdl)
```


The `skim()` output virtually speaks for itself. Our partitioned data for the initial model includes a total of 29 variables comprised of 10 numeric and 19 factor (or categorical) variables. There are a total of 10,000 rows. For the categorical variables, we have shown the names of each variable, total number of unique values for each, and the sum of each unique value for each variable respectively. Additionally, for our 10 numeric type variables, we are provided with each variable's name followed by the mean, median, standard deviation, minimum value, lower quartile (.25), upper quartile (.75), and maximum value respectively. This summary gives us an excellent feel for the selected features for our initial model.

</br>

### C3: Steps to Prepare the Data

Now, we will begin the process of preparing the dataset, starting with loading in the necessary libraries and reading-in the cleaned dataset.

```{r}
# Load in packages without messages
suppressMessages(library(tidyverse))
suppressMessages(library(skimr))
suppressMessages(library(Hmisc))
suppressMessages(library(broom))
suppressMessages(library(car))
suppressMessages(library(MASS))
suppressMessages(library(InformationValue))

# Read dataset in
df <- read.csv("./data/medical_clean.csv")

# Create custom skim if not already done
my_skim <- skim_with(
  base = sfl(),
  numeric = sfl(Mean = mean,
                Median = median,
                StDev = sd,
                Min = min,
                Q25 = ~ quantile(., probs = .25),
                Q75 = ~ quantile(., probs = .75),
                Max = max),
  factor = sfl(Unique_Values = n_unique,
               Value_Counts = top_counts),
  append = FALSE
)

# Start with a quick skim of the data for orientation and future reference
my_skim(df)
```

Immediately we will drop variables that, at this time, are unnecessary for our objective and keep the rest.

```{r}
# Partition dataset to include only features to be initially included in model
# Initial_days is reordered to first position for ease of reference
init_mdl <- df %>%
  dplyr::select(Initial_days,
                Area,
                Children:Services,
                Additional_charges)

# View new dataframe and assess data types
init_mdl %>%
  my_skim()
```

We will next need to convert the data types of many our variables. Since there are not any variables needing type class conversion other than those which are currently of the type character, we will simply select all of those variables at once and convert them to factor variables. This will ensure that the model handles the variables as intended.

```{r}
# Start with reformatting all chr variables as fct
init_mdl[sapply(init_mdl, 
                is.character)] <- lapply(init_mdl[sapply(init_mdl, 
                                                         is.character)], 
                                         as.factor)

# Reassess dataframe structure using skim()
my_skim(init_mdl)
```

This `skim()` view of the dataframe is quite useful. It shows the dataframe is comprised of two data types: factor and numeric. Thus, it appears we were successful in converting our columns to factors. The majority of the variables included are either dichotomous or otherwise nominal categorical variables. Now that we have completed our conversion process and validated the composition of our data, we may now proceed with the model selection process.

</br>

### C4: Visualizations

#### Univariate:

Now we will show all of our model's variables using both univariate and bivariate visualizations. We'll start with univariate histograms of the numeric variables, then we'll look at bar charts for all of our categorical variables.

```{r}
# Show histograms for all numeric variables
par(mfrow = c(3,4))
hist(init_mdl %>% 
     select_if(is.numeric))
```

```{r}
# Partition dichotomous Yes/No variables out for plot
dichotomous_vars <- init_mdl %>% 
  dplyr::select(where(~n_distinct(.) == 2))

# Show bar charts of all Yes/No variables
dichotomous_vars %>%
  gather() %>%
  count(key, value) %>% 
  ggplot(., aes(x = value, y = n)) +
  geom_bar(stat = "identity") +
  facet_wrap(~key, scales = "free", nrow = 3)
```

```{r}
# Partition non-dichotomous categorical variables
cat_vars <- init_mdl %>% 
  select_if(is.factor) %>% 
  dplyr::select(where(~n_distinct(.) != 2))

# Rename levels to shorter versions to fit plots
levels(cat_vars$Marital) <- c("Divorced",
                              "Married",
                              "Never",
                              "Sep",
                              "Widow")
levels(cat_vars$Initial_admin) <- c("Elective",
                                    "Observation",
                                    "Emergency")

```

```{r}
# Create panel of bar charts for cat_vars
par(mfrow = c(3,2))
barplot(table(cat_vars$Area), main = "Geographical Area")
barplot(table(cat_vars$Marital), main = "Marital Status")
barplot(table(cat_vars$Gender), main = "Gender")
barplot(table(cat_vars$Initial_admin), main = "Reason for Initial Admission")
barplot(table(cat_vars$Complication_risk), main = "Complication Risk")
barplot(table(cat_vars$Services), main = "Services Used")
```

</br>

#### Bivariate:

Now we will get a different view of the data using scatter and box plots to dive a little deeper.

```{r}
# Partition model data for only numeric variables
num_vars <- init_mdl %>% 
  select_if(is.numeric)

# Show scatter plot matrix of numeric variables
pairs(num_vars,
      col=init_mdl$ReAdmis)
```

Our target variable (`ReAdmis`) can be identified as the red in the above visualization. 


```{r}
# Boxplots of our ReAdmis target variable against some numerical variables
par(mfrow = c(2,3))
boxplot(Initial_days ~ ReAdmis, data = init_mdl)
boxplot(Children ~ ReAdmis, data = init_mdl)
boxplot(Age ~ ReAdmis, data = init_mdl)
boxplot(VitD_levels ~ ReAdmis, data = init_mdl)
boxplot(Income ~ ReAdmis, data = init_mdl)
boxplot(Doc_visits ~ ReAdmis, data = init_mdl)
```

</br>

### C5: Prepared Data Set

For prepared dataset, please see attached .csv file.
```{r}
write.csv(init_mdl, "./data/initmdl.csv")
```

___

</br>
</br>

## Part IV

___

### D1: Initial Model

The initial model will consist of essentially any potentially relevant independent variables. The initial model will then be evaluated and reduced using forward and backward stepwise selection. Considering the logistic regression equation is as follows:

$$
ln \left( \frac{ \hat{ p(x) } } {\left( 1 - \hat{ p(x) } \right) } \right) = \beta_0 + \beta_1x_i + \beta_2x_2 + ... + \beta_nx_n + e_i
$$

and that the number of coefficients is equal to the number of independent variables, our initial model looks something like this:

$$
\begin{align}
ln \left( \frac{ \hat{ p(ReAdmis) } } {\left( 1 - \hat{ p(ReAdmis) } \right) } \right) = \hat{\beta}_0 \\
    &+ \hat{\beta}_1 \text{AreaSuburban} \\
    &+ \hat{\beta}_2 \text{AreaUrban} \\
    &+ \hat{\beta}_3 \text{Children} \\
    &+ \hat{\beta}_4 \text{Age} \\
    &+ \hat{\beta}_5 \text{Income} \\
    &+ \hat{\beta}_6 \text{MaritalMarried} \\
    &+ \hat{\beta}_7 \text{MaritalNever Married} \\
    &+ \hat{\beta}_8 \text{MaritalSeparated} \\
    &+ \hat{\beta}_9 \text{MaritalWidowed} \\
    &+ \hat{\beta}_{10} \text{GenderMale} \\
    &+ \hat{\beta}_{11} \text{GenderNonbinary} \\
    &+ \hat{\beta}_{12} \text{Initial_days} \\
    &+ \hat{\beta}_{13} \text{VitD_levels} \\
    &+ \hat{\beta}_{14} \text{Doc_visits} \\
    &+ \hat{\beta}_{15} \text{Full_meals_eaten} \\
    &+ \hat{\beta}_{16} \text{Soft_drinkYes} \\
    &+ \hat{\beta}_{17} \text{Initial_adminObservation Admission} \\
    &+ \hat{\beta}_{18} \text{Initial_adminElective Admission} \\
    &+ \hat{\beta}_{19} \text{HighBloodYes} \\
    &+ \hat{\beta}_{20} \text{StrokeYes} \\
    &+ \hat{\beta}_{21} \text{Complication_riskLow} \\
    &+ \hat{\beta}_{22} \text{Complication_riskHigh} \\
    &+ \hat{\beta}_{23} \text{OverweightYes} \\
    &+ \hat{\beta}_{24} \text{ArthritisYes} \\
    &+ \hat{\beta}_{25} \text{DiabetesYes} \\
    &+ \hat{\beta}_{26} \text{BackPainYes} \\
    &+ \hat{\beta}_{27} \text{AnxietyYes} \\
    &+ \hat{\beta}_{28} \text{Allergic_rhinitisYes} \\
    &+ \hat{\beta}_{29} \text{Reflux_esophagitisYes} \\
    &+ \hat{\beta}_{30} \text{AsthmaYes} \\
    &+ \hat{\beta}_{31} \text{ServicesCT Scan} \\
    &+ \hat{\beta}_{32} \text{ServicesIntravenous} \\
    &+ \hat{\beta}_{33} \text{ServicesMRI} \\ 
    &+ \hat{\beta}_{34} \text{Additional_charges} + \hat{\epsilon}
\end{align}
$$

Though that equation is quite unwieldy, our initial model is designed to include the maximum amount of predictor features that can reasonably be included and potentially relate to the target feature. The model also includes factor variables broken-out as dummy variables, which adds to the length of the model. In terms of our multiple logistic regression formula in $\mbox{R}$, we've already cleaned and transformed our dataframe for the initial model, so the formula is much more concise: 
```
ReAdmis ~ .,
  data = init_mdl)
```

</br>

### D2: Justification of Model Reduction

Our variable selection process includes the use of stepwise elimination methods, forward and backward, to systematically reduce each model one variable at a time and finally arriving at an optimal group of variables. The stepwise selection methods are similar in approach, but each can result in substantively different models. Using both forward and backward, therefore, allow us to analyze and evaluate the resulting models from each and choose the model that works best for our initial business question/hypothesis test: what relates to and/or influences a patient's initial length of hospitalization.

Our feature selection criteria for each model reduction process will be the comparison of z-statistic p-value and AIC for individual predictors. Each step will either include or exclude the most or least useful variable respectively until no further benefit is observed. Then, the resulting reduced models will be compared against each other using the $AIC$ criteria, which is particularly useful for our logistic models, as well as examining the effective interpretation of coefficients. The model with the "best" combination of these measures (i.e. lowest $AIC$ and best explanation of coefficients) will ultimately be selected as our reduced model.

</br>

### D3: Reduced Logistic Regression Model

The reduced model is as follows:

$$
\begin{align}
ln \left( \frac{ \hat{ p(ReAdmis) } } {\left( 1 - \hat{ p(ReAdmis) } \right) } \right) = \hat{\beta}_0 \\
    &+ \hat{\beta}_1 \text{Initial_days} \\
    &+ \hat{\beta}_2 \text{Initial_adminEmergency Admission} \\
    &+ \hat{\beta}_3 \text{Initial_adminObservation Admission} \\
    &+ \hat{\beta}_4 \text{ServicesCT Scan} \\
    &+ \hat{\beta}_5 \text{ServicesIntravenous} \\
    &+ \hat{\beta}_6 \text{ServicesMRI} \\
    &+ \hat{\beta}_7 \text{StrokeYes} \\
    &+ \hat{\beta}_8 \text{Complication_riskLow} \\
    &+ \hat{\beta}_9 \text{Complication_riskMedium} \\
    &+ \hat{\beta}_{10} \text{ArthritisYes} \\
    &+ \hat{\beta}_{11} \text{AsthmaYes} \\
    &+ \hat{\beta}_{12} \text{AnxietyYes} \\
    &+ \hat{\beta}_{13} \text{HighBloodYes} \\
    &+ \hat{\beta}_{14} \text{DiabetesYes} \\
    &+ \hat{\beta}_{15} \text{Children} \\
    &+ \hat{\beta}_{16} \text{Reflux_esophagitisYes} \\
    &+ \hat{\beta}_{17} \text{Allergic_rhinitisYes}  + \hat{\epsilon_i}\\
\end{align}
$$


Our multiple regression formula in $\mbox{R}$, is: 
```
ReAdmis ~ Initial_days +
          Children +
          Initial_admin +
          HighBlood +
          Stroke +
          Complication_risk +
          Arthritis +
          Diabetes +
          Anxiety +
          Allergic_rhinitis +
          Reflux_esophagitis +
          Asthma +
          Services,
          data = init_mdl)
```

</br>

___

</br>
</br>

## Part V

___

### E1: Model Comparison

As mentioned above, the process by which the multiple regression model was reduced and selected was by using forward and backward stepwise selection. Using the `MASS` package, we were able to run the selection process as an iterative function and analyze the results. The initial model was overfit with very little use to begin with. 

The forward selection process started by analyzing an essentially empty model, measured the impact on the z-statistic p-values and individual contribution to $AIC$ values for all features included in the full, initial model, and added the single feature with the most statistically significant contribution to the model at each step. This process was repeated until no further benefit was observed by adding an additional feature. This resulted in 13 statistically significant features included in the final model. 

The backward elimination process used the same evaluation criterion (z-statistic p-value and individual $AIC$ contribution). The process is simply the reverse of forward elimination. Analysis of the initial model's features resulted in the identification of the least statistically significant variable which was subsequently removed from the model. This step was repeated, each time removing the next-least statistically significant variable until no further benefit was observed. Backward elimination likewise concluded with 13 statistically significant features included in the optimal model.

The final output of the two models includes, perhaps most notably, their $AIC$. The models were then compared by this metric and the logical explanation of coefficients. The optimal model for both selection processes resulted in identical feature selection. Typically, should forward and backward selection resulted in different models, the model with the best combination of the above ($AIC$ and logical/useful coefficients) would have been selected. It's important to note that an $AIC$ delta of greater than 2.0 would be sufficient to reject as inferior.

>https://cals.arizona.edu/classes/wfsc578/Symonds%20and%20Moussali%202011.%20A%20brief%20guide%20to%20model%20selection.pdf
>Burnham KP, Anderson DR (2002) Model selection and multimodelinference, 2nd edn. Springer, New York


</br>
</br>

### E2: Output and Calculations

Consider the following summary outputs for both models including the Confusion Matrix and misclassification error rate of both.

```{r}
# Summary of the forward model
summary(stepfwd)
```


```{r}
# Create predicted values based on stepfwd
predicted <- predict(stepfwd,
                     init_mdl,
                     type = "response")

# Convert target var (ReAdmis) to binary 1/0 vals
predmdl <- init_mdl
predmdl$ReAdmis <- ifelse(predmdl$ReAdmis == "Yes",
                          1,
                          0)

# Compute optimal prob cutoff score
optimal <- optimalCutoff(predmdl$ReAdmis,
                         predicted)[1]

# Create confusion matrix of actuals and predicted values
as.matrix(confusionMatrix(predmdl$ReAdmis,
                predicted))

# Declare misclassification error
misClassError(predmdl$ReAdmis,
              predicted,
              threshold=optimal)
```




```{r}
# Summary of the backward elimination model
summary(stepbck)
```

```{r}
# Create predicted values based on stepbck
predicted <- predict(stepbck,
                     init_mdl,
                     type = "response")

# Convert target var (ReAdmis) to binary 1/0 vals
predmdl <- init_mdl
predmdl$ReAdmis <- ifelse(predmdl$ReAdmis == "Yes",
                          1,
                          0)

# Compute optimal prob cutoff score
optimal <- optimalCutoff(predmdl$ReAdmis,
                         predicted)[1]

# Create confusion matrix of actuals and predicted values
as.matrix(confusionMatrix(predmdl$ReAdmis,
                predicted))

# Declare misclassification error
misClassError(predmdl$ReAdmis,
              predicted,
              threshold=optimal)
```


### Predictions using reduced model:

```{r}
# Create a dataframe of 20 predictor vectors
p_Initial_days <- seq(min(init_mdl$Initial_days),
                      max(init_mdl$Initial_days),
                      by = (max(init_mdl$Initial_days) - min(init_mdl$Initial_days)) / 19)
p_Initial_admin <- rep_len(levels(init_mdl$Initial_admin),
                           length.out = 20)
p_Services <- rep_len(levels(init_mdl$Services),
                           length.out = 20)
p_Stroke <- rep_len(levels(init_mdl$Stroke),
                       length.out = 20)
p_Complication_risk <- rep_len(levels(init_mdl$Complication_risk),
                               length.out = 20)
p_Arthritis <- rep_len(levels(init_mdl$Arthritis),
                       length.out = 20)
p_Asthma <- rep_len(levels(init_mdl$Asthma),
                       length.out = 20)
p_Anxiety <- rep_len(levels(init_mdl$Anxiety),
                     length.out = 20)
p_HighBlood <- rep_len(levels(init_mdl$HighBlood),
                     length.out = 20)
p_Diabetes <- rep_len(levels(init_mdl$Diabetes),
                     length.out = 20)
p_Children <- sample(init_mdl$Children,
                     20,
                     replace = TRUE)
p_Reflux_esophagitis <- rep_len(levels(init_mdl$Reflux_esophagitis),
                     length.out = 20)
p_Allergic_rhinitis <- rep_len(levels(init_mdl$Allergic_rhinitis),
                     length.out = 20)

p_mdl <- data.frame(Initial_days = p_Initial_days,
                    Initial_admin = p_Initial_admin,
                    Services = p_Services,
                    Stroke = p_Stroke,
                    Complication_risk = p_Complication_risk,
                    Arthritis = p_Arthritis,
                    Asthma = p_Asthma,
                    Anxiety = p_Anxiety,
                    HighBlood = p_HighBlood,
                    Diabetes = p_Diabetes,
                    Children = p_Children,
                    Reflux_esophagitis = p_Reflux_esophagitis,
                    Allergic_rhinitis = p_Allergic_rhinitis)

# Add predictions column p_ReAdmis
p_mdl$p_ReAdmis <- predict(stepfwd,
                           new = p_mdl,
                           type = "response")

p_mdl
```

</br>

### E3: Code


```{r}
# Initial/upper logit model
logitfull <- glm(ReAdmis ~ .,
                data = init_mdl,
                family = binomial)

# Lower logit model
logitlow <- glm(ReAdmis ~ 1,
                data = init_mdl,
                family = binomial)

```



```{r}
# Forward stepwise model selection (suppress perfect separation warnings)
stepfwd <- suppressWarnings(stepAIC(logitlow,
                                    scope = list(lower = logitlow,
                                                 upper = logitfull)))
```



```{r}
# Backward stepwise model selection (suppress perfect separation warnings)
stepbck <- suppressWarnings(stepAIC(logitfull,
                                    direction = "backward"))
```



```{r}
# Show forward-reduced model summary
summary(stepfwd)
```




```{r}
# Show backward-reduced model summary
summary(stepbck)
```



```{r}
# Reduced Model (stepbck)
reduced_mdl <- glm(ReAdmis ~ Initial_days +
                     Initial_admin +
                     Services +
                     Stroke +
                     Complication_risk +
                     Arthritis +
                     Asthma +
                     Anxiety +
                     HighBlood +
                     Diabetes +
                     Children +
                     Reflux_esophagitis +
                     Allergic_rhinitis,
                data = init_mdl,
                family = binomial)
```


</br>

___

</br>
</br>

## Part VI

___

### F1: Results

Multiple logistic regression analyses were performed to interrogate the relationship between the binary response variable, `ReAdmis`, and a group of various, potentially related predictor variables as the initial model. Subsequently, two variable selection methods were employed, namely forward selection and backward elimination, to iteratively reduce the initial model. The regression equation for the reduced model is:
$$
\begin{align}
ln \left( \frac{ \hat{ p(ReAdmis) } } {\left( 1 - \hat{ p(ReAdmis) } \right) } \right) = \hat{\beta}_0 \\
    &+ \hat{\beta}_1 \text{Initial_days} \\
    &+ \hat{\beta}_2 \text{Initial_adminEmergency Admission} \\
    &+ \hat{\beta}_3 \text{Initial_adminObservation Admission} \\
    &+ \hat{\beta}_4 \text{ServicesCT Scan} \\
    &+ \hat{\beta}_5 \text{ServicesIntravenous} \\
    &+ \hat{\beta}_6 \text{ServicesMRI} \\
    &+ \hat{\beta}_7 \text{StrokeYes} \\
    &+ \hat{\beta}_8 \text{Complication_riskLow} \\
    &+ \hat{\beta}_9 \text{Complication_riskMedium} \\
    &+ \hat{\beta}_{10} \text{ArthritisYes} \\
    &+ \hat{\beta}_{11} \text{AsthmaYes} \\
    &+ \hat{\beta}_{12} \text{AnxietyYes} \\
    &+ \hat{\beta}_{13} \text{HighBloodYes} \\
    &+ \hat{\beta}_{14} \text{DiabetesYes} \\
    &+ \hat{\beta}_{15} \text{Children} \\
    &+ \hat{\beta}_{16} \text{Reflux_esophagitisYes} \\
    &+ \hat{\beta}_{17} \text{Allergic_rhinitisYes}  + \hat{\epsilon_i}\\
\end{align}
$$

As mentioned above, the logistic regression equation is as follows:

$$
ln \left( \frac{ \hat{ p(x) } } {\left( 1 - \hat{ p(x) } \right) } \right) = \beta_0 + \beta_1x_i + \beta_2x_2 + ... + \beta_nx_n + e_i
$$
 
Therefore, a potential interpretation of this model and its coefficients could be:
$$
\begin{align}
ln \left( \frac{ \hat{ p(ReAdmis) } } {\left( 1 - \hat{ p(ReAdmis) } \right) } \right) = \\    

    & \text{-80.77379} \\   
    &+ \text{Initial length of hospitalization}*(1.47447) \\
    &+ \text{2.52225 (if patient's initial admission was emergency, otherwise 0)} \\
    &+ \text{0.77122 (if patient's initial admission was an observation, otherwise 0)} \\
    &+ \text{1.58311 (if patient received CT scan services, otherwise 0)} \\
    &+ \text{0.01598 (if patient's received intravenous services, otherwise 0)} \\
    &+ \text{2.74402 (if patient's received MRI services, otherwise 0)} \\
    &+ \text{1.62981 (if patient has had a stroke, otherwise 0)} \\
    &- \text{1.85203 (if patient's complication risk was low, otherwise 0)} \\
    &- \text{0.33849 (if patient's complication risk was medium, otherwise 0)} \\
    &- \text{1.34063 (if patient has arthritis, otherwise 0)} \\
    &- \text{1.34759 (if patient has asthma, otherwise 0)} \\
    &- \text{1.02489 (if patient has anxiety, otherwise 0)} \\
    &+ \text{0.86692 (if patient has high blood pressure, otherwise 0)} \\
    &+ \text{0.46011 (if patient has diabetes, otherwise 0)} \\
    &+ \text{Number of children}*(0.09512) \\
    &- \text{0.40251 (if patient has reflux esophagitis, otherwise 0)} \\
    &- \text{0.33276 (if patient has allergic rhinitis, otherwise 0)} \\
\end{align}
$$


However, in order to derive real meaning from the model, it's useful to convert the coefficients of the model from log-odds to probability. That conversion looks like the following:
$$
\hat{p(x)} = {\frac{exp(\beta_0 + \beta_1x_i + \beta_2x_2 + ... + \beta_nx_n + e_i)}{1 + exp (\beta_0 + \beta_1x_i + \beta_2x_2 + ... + \beta_nx_n + e_i)}}
$$
<!-- •  the statistical and practical significance of the model -->
Statistically, this model is quite reliable in predicting patient readmission based on the given parameters. The confusion matrix above details a highly accurate classification in terms of true positive and negative predictions. Practically, however, the model relies heavily on `Initial_days` as a predictor. This seems to indicate the longer a patient has been initially hospitalized, the higher the likelihood of their eventual readmission. This cannot be a surprise to those familiar with healthcare outcomes. Therefore, the value of the model is somewhat diminished, in my opinion, due to how heavily it relies on that single feature. Nonetheless, the model on the whole is not overly burdensome and it is quite accurate, at least with regard to the dataset at hand.

Additionally, it should be noted that the `Initial_days` data is less than ideal given that its distribution is not normal. If anything, the distribution seems to take a bimodal shape and seems to have an odd cutoff point with no major outliers on the high end. It's possible the skew as well as the shape of the data caused the model to produce a "perfect separation" warning when fitting any combination of features including `Initial_days` or simply `ReAdmis ~ Initial_days` itself.


</br>

### F2: Recommendations
<!-- Recommend a course of action based on your results. -->
As a result of the preceding analysis, it is recommended that leadership consider implementing policy and procedure utilizing this logistic regression model to estimate a patient's potential probability of readmission. This model can be used as a limited, additional tool to estimate, and attempt to mitigate, readmission risk which would likely lead to better outcomes overall.

Furthermore, it is recommended that additional, prospective analysis and data capture be performed to better understand the distribution, as well as what potential drivers could be behind the distribution, of the `Initial_days` data. Also, other continuous variables of interest in future data capture could include various lab values such as those found in blood gas analysis, complete blood count panel, comprehensive metabolic panel, etc. As these laboratory values are obtained frequently and during hospitalization, and in some cases during prior doctor visits, and are highly indicative of a patient's status, it seems likely that having access to these data would allow for more useful, predictive models.

___

</br>
</br>
</br>


### I: Sources

___

