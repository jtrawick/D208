---
title: "D208 - Predictive Modeling"
output: 
  html_notebook: 
    toc: yes
    theme: lumen
---

</br>

------------------------------------------------------------------------

#### **Performance Assessment - Task 1: Multiple Regression for Predictive Modeling**

*Medical Readmission Data Set (Clean)*

------------------------------------------------------------------------

</br>

## Part I
___
### A1: Research Question

The central research question addressed by this analysis is to determine:

>What variables from the medical dataset influence a patient's initial total days spent hospitalized (`Initial_days`)? 

In terms of hypothesis testing, our null hypothesis ($H_0$) is:

>No combination of variables included in the medical dataset influence initial length of stay (`Initial_days`) in any statistically significant way.

Additionally, our alternate hypothesis ($H_1$) is:

>Some combination of variables included in the medical dataset influence initial length of stay (`Initial_days`) in a statistically significant way.

</br>

### A2: Objectives and Goals

The primary goal of the following analysis is to determine what variables, if any, contribute positively to a patient's initial number of days hospitalized (`Initial_days`). This will be assessed using the $\mbox{R}$ programming language and using the technique of multiple linear regression to identify causal relationships between independent variables and the target variable.

___
</br>
</br>

## Part II
___
### B1: Summary of Assumptions

The following are basic assumptions of multiple linear regression:

-   The assumption of a linear relationship between the explanatory variables and the target variable
-   The assumption that residual values are normally distributed
-   The assumption of non-multicollinearity of explanatory variables
-   The assumption that residual values are homoscedastic

</br>

### B2: Tool Benefits

The programming language of choice for this analysis, as previously mentioned, will be the $\mbox{R}$ programming language. Previously, this author has used Python to perform cleaning, transformation, and analysis. Python has been more than up to the task. However, $\mbox{R}$ happens to handle the process of regression analysis and model selection exceptionally well and thus became the self-evident choice for multiple regression analysis. In particular, the built-in functions of the base $\mbox{R}$ language, as used to fit models, are incredibly simple to navigate and equally as easy to demonstrate. Additionally, the reduction process using the `ols_step_backward_p()` function, as another example, made the task of choosing which programming language to use for this project even easier. It is with good reason that $\mbox{R}$ has such a stellar reputation for handling regression models.

</br>

### B3: Appropriate Technique

Multiple regression is an appropriate technique to use to accomplish our goal of finding which variables contribute to a longer initial length of stay (`Initial_days`) for several reasons. Firstly, the dataset we will analyze contains 50 variables in total, each of disparate significance and utility. While some variable do not require much thought prior to elimination, others are not quite as straightforward. Therefore, running them through a multiple regression model prior to discarding them is an appropriate course of action. Additionally, multiple regression will allow us to see the significance of each variable's discrete contribution to the target variable as well as the interaction between explanatory variables themselves. Overall, regarding our objective, multiple regression is more than up to the task and will adequately suit our objectives.

___
</br>
</br>

## Part III

___

### C1: Data Goals

The process we will need to take to prepare the data for model selection is relatively minor, given that the raw dataset used in this project has already been cleaned in a prior project (see project D206 - Data Cleaning). Using the pre-cleaned dataset, we will first partition the data to include only those variables we intend to feed into our initial model. Because the model selection process we will use is backward-oriented, this initial model will include all features that could possibly have a relationship to the target variable of initial length of hospitalization (`Initial_days`). This will include a mix of numeric and categorical variables. 

Next, we will need to ensure that the data type of each variable is appropriate for that kind of feature. For example, we will determine which categorical variables are nominal and which would benefit from ordinal encoding. Once the dataset for the initial model has been partitioned and transformed (or converted to the right type, at least), we will look over the dataset to ensure that we have not created any problems in the process such as silently introducing null values.

</br>

### C2: Summary Statistics

In order to get the best understanding of the selected features and their measures of central tendency, we will use an amazing library called `skimr` which does a phenomenal job of not only providing a great default summary view of the entire dataframe, but also allows one to customize the output. First, we will fashion a version of the skim function purpose-built for our needs here and print the output.

<!-- USE FOR FINAL DOCUMENT -->
<!-- ```{r} -->
<!-- # Set custom skim() for C2: Summary Statistics -->
<!-- # For numeric include mean, median, stdev, min, Q25, Q75, and max -->
<!-- # For factor include count of unique values and value counts for each -->
<!-- my_skim <- skim_with( -->
<!--   base = sfl(), -->
<!--   numeric = sfl(Mean = mean, -->
<!--                 Median = median, -->
<!--                 StDev = sd, -->
<!--                 Min = min, -->
<!--                 Q25 = ~ quantile(., probs = .25), -->
<!--                 Q75 = ~ quantile(., probs = .75), -->
<!--                 Max = max), -->
<!--   factor = sfl(Unique_Values = n_unique, -->
<!--                Value_Counts = top_counts), -->
<!--   append = FALSE -->
<!-- ) -->

<!-- # Call new skim format -->
<!-- my_skim(init_mdl) -->
<!-- ``` -->


The `skim()` output virtually speaks for itself. Our partitioned data for the initial model includes a total of 29 variables comprised of 10 numeric and 19 factor (or categorical) variables. There are a total of 10,000 rows. For the categorical variables, we have shown the names of each variable, total number of unique values for each, and the sum of each unique value for each variable respectively. Additionally, for our 10 numeric type variables, we are provided with each variable's name followed by the mean, median, standard deviation, minimum value, lower quartile (.25), upper quartile (.75), and maximum value respectively. This summary gives us an excellent feel for the selected features for our initial model.

</br>

### C3: Steps to Prepare the Data

Now, we will begin the process of preparing the dataset, starting with loading in the necessary libraries and reading-in the cleaned dataset.

```{r}
# Load in packages without messages
suppressMessages(library(tidyverse))
suppressMessages(library(skimr))
suppressMessages(library(Hmisc))
suppressMessages(library(broom))
suppressMessages(library(olsrr))
suppressMessages(library(car))
```

```{r}
# Read dataset in
df <- read.csv("./data/medical_clean.csv")

# Start with a quick skim of the data for orientation and future reference
(df)
```

Immediately we will drop variables that, at this time, are unnecessary for our objective and keep the rest.

```{r}
# Partition dataset to include only features to be initially included in model
# Initial_days is reordered to first position for ease of reference
init_mdl <- df %>%
  select(Initial_days,
         Area,
         Children:Services,
         Additional_charges)

# View new dataframe and assess dtypes
init_mdl %>%
  skim()
```

We will next need to convert the data types of many our variables. Since there are not any variables needing type class conversion other than those which are currently of the type character, we will simply select all of those variables at once and convert them to factor variables. This will ensure that the model handles the variables as intended.

```{r}
# Start with reformatting all chr variables as fct
init_mdl[sapply(init_mdl, is.character)] <- lapply(init_mdl[sapply(init_mdl, is.character)], as.factor)

# Reassess dataframe structure using skim()
skim_without_charts(init_mdl)
```

This `skim()` view of the dataframe is quite useful. It shows the dataframes is comprised of two data types: factor and numeric. Thus, it appears we were successful in converting our columns to factors. The majority of the variables included are either dichotomous or otherwise nominal categorical variables. We do, however, have a couple of variables that likely would benefit from changing the reference category. The previous code was convenient for converting a relatively large group of variables to factors all at once, but does not consider levels. Thus, we will take to changing the reference level for two variables (Initial_admin and Complication_risk) below. First, let's just verify our assumptions by accessing these variables' levels to determine which category comes first. If our model is attempting to discover what features contribute to a longer initial length of hospitalization, it would be helpful to have the least likely contributor be the reference category, such that the coefficients are positively oriented. Based on measures of central tendency for these two variables, Emergency Admission for `Initial_admin` and Medium `Complication_risk` have the lowest combination of median, mean and standard deviation and will be reset as the reference category for their respective variable.

```{r}
# Access levels() for each variable
levels(init_mdl$Initial_admin)
levels(init_mdl$Complication_risk)
```

OK, our assumptions about the order were valid. Now, let's fix them.

```{r}
# Re-level ordinal categorical variables for Initial_admin
init_mdl$Initial_admin <- factor(init_mdl$Initial_admin,
                                 levels = c("Emergency Admission",
                                            "Observation Admission",
                                            "Elective Admission"))
# Re-level ordinal categorical variables for Complication_risk
init_mdl$Complication_risk <- factor(init_mdl$Complication_risk,
                                 levels = c("Medium",
                                            "Low",
                                            "High"))
```

Finally, we will check again to make sure the above worked as intended.

```{r}
# Access levels() for each variable
levels(init_mdl$Initial_admin)
levels(init_mdl$Complication_risk)
```

Now that we have validated our conversion process and the composition of our data, we can now proceed with the model selection process. 

</br>

### C4: Visualizations

#### Univariate:

Now we will show all of our model's variables using both univariate and bivariate visualizations. We'll start with univariate histograms of the numeric variables, then we'll look at bar charts for all of our categorical variables.

```{r}
# Show histograms for all numeric variables
par(mfrow = c(3,4))
hist(init_mdl %>% 
     select_if(is.numeric))
```

```{r}
# Partition dichotomous Yes/No variables out for plot
dichotomous_vars <- init_mdl %>% 
  select(where(~n_distinct(.) == 2))

# Show bar charts of all Yes/No variables
dichotomous_vars %>%
  gather() %>%
  count(key, value) %>% 
  ggplot(., aes(x = value, y = n)) +
  geom_bar(stat = "identity") +
  facet_wrap(~key, scales = "free", nrow = 3)
```

```{r}
# Partition non-dichotomous categorical variables
cat_vars <- init_mdl %>% 
  select_if(is.factor) %>% 
  select(where(~n_distinct(.) != 2))

# Rename levels to shorter versions to fit plots
levels(cat_vars$Marital) <- c("Divorced",
                              "Married",
                              "Never",
                              "Sep",
                              "Widow")
levels(cat_vars$Initial_admin) <- c("Elective",
                                    "Observation",
                                    "Emergency")

```

```{r}
# Create panel of bar charts for cat_vars
par(mfrow = c(3,2))
barplot(table(cat_vars$Area), main = "Geographical Area")
barplot(table(cat_vars$Marital), main = "Marital Status")
barplot(table(cat_vars$Gender), main = "Gender")
barplot(table(cat_vars$Initial_admin), main = "Reason for Initial Admission")
barplot(table(cat_vars$Complication_risk), main = "Complication Risk")
barplot(table(cat_vars$Services), main = "Services Used")
```

</br>

#### Bivariate:

Now we will get a different view of the data using scatter and box plots to dive a little deeper.

```{r}
# Partition model data for only numeric variables
num_vars <- init_mdl %>% 
  select_if(is.numeric)

# Show scatterplot matrix of numeric variables
pairs(num_vars, )
```

```{r}
# Boxplots of our target variable against some categorical variables
par(mfrow = c(2,3))
boxplot(Initial_days ~ Complication_risk, data = init_mdl)
boxplot(Initial_days ~ Gender, data = init_mdl)
boxplot(Initial_days ~ Overweight, data = init_mdl)
boxplot(Initial_days ~ Anxiety, data = init_mdl)
boxplot(Initial_days ~ ReAdmis, data = init_mdl)
boxplot(Initial_days ~ Initial_admin, data = init_mdl)
```

</br>

### C5: Prepared Data Set

For prepared dataset, please see attached .csv file.
```{r}
write.csv(init_mdl, "./data/initmdl.csv")
```

___

</br>
</br>

## Part IV

___

### D1: Initial Model

The initial model will consist of essentially any potentially relevant independent variables. The initial model will then be evaluated and reduced using a combination of backward and stepwise selection. Considering the linear regression equation is as follows:

$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + \hat{\beta}_2 x_i + \hat{\epsilon}_i$

and that the number of coefficients is equal to the number of independent variables, our initial model looks something like this:

$$
\begin{align}
\hat{\text{Initial_days}}_i = \hat{\beta}_0
    &+ \hat{\beta}_1 \text{AreaSuburban}_i \\
    &+ \hat{\beta}_2 \text{AreaUrban}_i \\
    &+ \hat{\beta}_3 \text{Children}_i \\
    &+ \hat{\beta}_4 \text{Age}_i \\
    &+ \hat{\beta}_5 \text{Income}_i \\
    &+ \hat{\beta}_6 \text{MaritalMarried}_i \\
    &+ \hat{\beta}_7 \text{MaritalNever Married}_i \\
    &+ \hat{\beta}_8 \text{MaritalSeparated}_i \\
    &+ \hat{\beta}_9 \text{MaritalWidowed}_i \\
    &+ \hat{\beta}_{10} \text{GenderMale}_i \\
    &+ \hat{\beta}_{11} \text{GenderNonbinary}_i \\
    &+ \hat{\beta}_{12} \text{ReAdmisYes}_i \\
    &+ \hat{\beta}_{13} \text{VitD_levels}_i \\
    &+ \hat{\beta}_{14} \text{Doc_visits}_i \\
    &+ \hat{\beta}_{15} \text{Full_meals_eaten}_i \\
    &+ \hat{\beta}_{16} \text{Soft_drinkYes}_i \\
    &+ \hat{\beta}_{17} \text{Initial_adminObservation Admission}_i \\
    &+ \hat{\beta}_{18} \text{Initial_adminElective Admission}_i \\
    &+ \hat{\beta}_{19} \text{HighBloodYes}_i \\
    &+ \hat{\beta}_{20} \text{StrokeYes}_i \\
    &+ \hat{\beta}_{21} \text{Complication_riskLow}_i \\
    &+ \hat{\beta}_{22} \text{Complication_riskHigh}_i \\
    &+ \hat{\beta}_{23} \text{OverweightYes}_i \\
    &+ \hat{\beta}_{24} \text{ArthritisYes}_i \\
    &+ \hat{\beta}_{25} \text{DiabetesYes}_i \\
    &+ \hat{\beta}_{26} \text{BackPainYes}_i \\
    &+ \hat{\beta}_{27} \text{AnxietyYes}_i \\
    &+ \hat{\beta}_{28} \text{Allergic_rhinitisYes}_i \\
    &+ \hat{\beta}_{29} \text{Reflux_esophagitisYes}_i \\
    &+ \hat{\beta}_{30} \text{AsthmaYes}_i \\
    &+ \hat{\beta}_{31} \text{ServicesCT Scan}_i \\
    &+ \hat{\beta}_{32} \text{ServicesIntravenous}_i \\
    &+ \hat{\beta}_{33} \text{ServicesMRI}_i \\ 
    &+ \hat{\beta}_{34} \text{Additional_charges}_i + \hat{\epsilon_i}
\end{align}
$$

Though that equation is quite unwieldy, our initial model is designed to include the maximum amount of predictor features that can reasonably be included and potentially relate to the target feature. The model also includes factor variables broken-out as dummy variables, which adds to the length of the model. In terms of our multiple regression formula in $\mbox{R}$, we've already cleaned and transformed our dataframe for the initial model, so the formula is much more concise: 
```
Initial_days ~ .,
               data = init_mdl)
```

</br>

### D2: Justification of Model Reduction
<!-- Justify a statistically based variable selection procedure and a model evaluation metric to reduce the initial model in a way that aligns with the research question. -->

Our variable selection process includes the use of three stepwise elimination methods, namely backward, forward, and bidirectional stepwise elimination. The three selection methods are similar in approach, but each can result in substantively different models. Using all three will, therefore, allow us to analyze and evaluate the resulting models from each and choose the model that works best for our initial business question/hypothesis test: what relates to and/or influences a patient's initial length of hospitalization.

Our feature selection criteria for each model reduction process will be the comparison of t-statistic p-value and whether or not it meets the threshold of <0.1 for a given feature to be selected. Then, we will compare and evaluate the three resulting models using their corresponding Root Mean Square Error (RMSE), $R^2$, and F-Statistic. The model with the "best" combination of these measures (i.e. lowest RMSE, highest $R^2$, and lowest F-Statistic) will be selected as our reduced model.

</br>

### D3: Reduced Multiple Regression Model
<!-- Provide a reduced multiple regression model that includes both categorical and continuous variables. -->
The reduced model is as follows:

$$
\begin{align}
\hat{\text{Initial_days}}_i = \hat{\beta}_0
    &+ \hat{\beta}_1 \text{Age}_i \\
    &+ \hat{\beta}_2 \text{ReAdmisYes}_i \\
    &+ \hat{\beta}_3 \text{Initial_adminObservation Admission}_i \\
    &+ \hat{\beta}_4 \text{Initial_adminElective Admission}_i \\
    &+ \hat{\beta}_5 \text{Complication_riskLow}_i \\
    &+ \hat{\beta}_6 \text{Complication_riskHigh}_i \\
    &+ \hat{\beta}_7 \text{ArthritisYes}_i \\
    &+ \hat{\beta}_8 \text{AnxietyYes}_i \\
    &+ \hat{\beta}_9 \text{Additional_charges}_i + \hat{\epsilon_i}
\end{align}
$$

Our multiple regression formula in $\mbox{R}$, is: 
```
Initial_days ~ Age + 
               ReAdmis + 
               Initial_admin + 
               Complication_risk + 
               Arthritis + 
               Anxiety + 
               Additional_charges, 
               data = init_mdl)
```

</br>

___

</br>
</br>

## Part V

___

### E1: Model Comparison

<!-- Explain your data analysis process by comparing the initial and reduced multiple regression models, including the following elements: -->

<!-- •  the logic of the variable selection technique -->

<!-- •  the model evaluation metric -->

<!-- •  a residual plot -->


</br>

### E2: Output and Calculations

<!-- Provide the output and any calculations of the analysis you performed, including the model’s residual error. -->


</br>

### E3: Code

<!-- Provide the code used to support the implementation of the multiple regression models. -->

```{r}
# Assign full initial model (using all previously selected vars) to mdl_fit
mdl_fit <- lm(Initial_days ~ .,
              data = init_mdl)
# View summary of initial model
summary(mdl_fit)
```

<!-- While this view is the most useful for our purposes, and given that our initial model contains 29 features, it would be helpful to be able to view the summary output for the model in a way that allows us to sort by our selection criteria. To do this, we will use the `broom` package to create a dataframe from the `summary(mdl_fit)` output and sort by the P Value column. Our selection criteria is such that features with a t-statistic p-value of greater than 0.05 (95% confidence level of statistical significance) and, consequently, a high t-statistic value, will be eliminated from the model. Each step in the selection process, the coefficient with the highest t-statistic value and p-value will be considered for elimination unless there is a compelling reason to make an exception. -->

<!-- # ```{r} -->
<!-- # # Create dataframe mdl_output from summary(mdl_fit) using tidy() -->
<!-- # mdl_output <- as.data.frame(tidy(mdl_fit)) -->
<!-- #  -->
<!-- # # Sort mdl_output coefficients by p-value from largest to smallest -->
<!-- # mdl_output <- mdl_output[order(mdl_output$p.value, decreasing = TRUE), ] -->
<!-- # mdl_output -->
<!-- # ``` -->

<!-- # ```{r} -->
<!-- # # Create empty list for excluded variables -->
<!-- # ex_vars <- c() -->
<!-- #  -->
<!-- # # Pass coefficient to be eliminated into ex_vars -->
<!-- # ex_vars <- append(ex_vars, -->
<!-- #                   mdl_output[1,1]) -->
<!-- #  -->
<!-- # # Create formula as mdl_form to be passed into mdl_fit -->
<!-- # mdl_form <- as.formula(paste("Initial_days", -->
<!-- #                              paste(ex_vars, -->
<!-- #                                    collapse = " - "), -->
<!-- #                              sep = " ~ . -")) -->
<!-- #  -->
<!-- # # Adjust original model using list of excluded vars -->
<!-- # mdl_fit <- lm(mdl_form, -->
<!-- #               data = init_mdl) -->
<!-- #  -->
<!-- # # View summary of initial model -->
<!-- # summary(mdl_fit) -->
<!-- # ``` -->

<!-- # ```{r} -->
<!-- # # Create dataframe mdl_output from summary(mdl_fit) using tidy() -->
<!-- # mdl_output <- as.data.frame(tidy(mdl_fit)) -->
<!-- #  -->
<!-- # # Sort mdl_output coefficients by p-value from largest to smallest -->
<!-- # mdl_output <- mdl_output[order(mdl_output$p.value, decreasing = TRUE), ] -->
<!-- # mdl_output -->
<!-- # ``` -->



<!-- # ```{r} -->
<!-- # library() -->
<!-- # newmdl <- regsubsets(Initial_days ~ ., init_mdl, method = "backward") -->
<!-- # summary(newmdl) -->
<!-- # ``` -->


```{r}
mdl_back <- ols_step_backward_p(mdl_fit, details = TRUE, prem = .1)

mdl_fwd <- ols_step_forward_p(mdl_fit, details = TRUE, penter = .1)

mdl_both <- ols_step_both_p(mdl_fit, details = TRUE, pent = .1, prem = .1)
```


```{r}
# Summary of the backward model
summary(mdl_back$model)
```


```{r}
# Summary of the forward model
summary(mdl_fwd$model)
```


```{r}
# Summary of the bidirectional model
summary(mdl_both$model)
```


```{r}
plot(mdl_back$model)
```


```{r}
plot(mdl_fwd$model)
```


```{r}
plot(mdl_both$model)
```



</br>

___

</br>
</br>

## Part VI

___

### F1: Results

<!-- Discuss the results of your data analysis, including the following elements: -->

<!-- •  a regression equation for the reduced model -->

<!-- •  an interpretation of coefficients of the statistically significant variables of the model -->

<!-- •  the statistical and practical significance of the model -->

<!-- •  the limitations of the data analysis -->


</br>

### F2: Recommendations

<!-- Recommend a course of action based on your results. -->

___

</br>
</br>
</br>


### I: Sources

Anova(mdl_both$model, singular.ok = TRUE)
___
