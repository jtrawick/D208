---
title: "D208 - Predictive Modeling"
output: 
  html_notebook: 
    toc: yes
    theme: lumen
---

</br>

------------------------------------------------------------------------

#### **Performance Assessment - Task 1: Multiple Regression for Predictive Modeling**

*Medical Readmission Data Set (Clean)*

------------------------------------------------------------------------

</br>

## Part I
___
### A1: Research Question

The central research question addressed by this analysis is to determine:

>What variables from the medical dataset influence a patient's initial total days spent hospitalized (`Initial_days`)? 

In terms of hypothesis testing, our null hypothesis ($H_0$) is:

>No combination of variables included in the medical dataset influence initial length of stay (`Initial_days`) in any statistically significant way.

Additionally, our alternate hypothesis ($H_1$) is:

>Some combination of variables included in the medical dataset influence initial length of stay (`Initial_days`) in a statistically significant way.

</br>

### A2: Objectives and Goals

The primary goal of the following analysis is to determine what variables, if any, contribute positively to a patient's initial number of days hospitalized (`Initial_days`). This will be assessed using the $\mbox{R}$ programming language and using the technique of multiple linear regression to identify causal relationships between independent variables and the target variable.

___
</br>
</br>

## Part II
___
### B1: Summary of Assumptions

The following are basic assumptions of multiple linear regression:

-   The assumption of a linear relationship between the explanatory variables and the target variable
-   The assumption that residual values are normally distributed
-   The assumption of non-multicollinearity of explanatory variables
-   The assumption that residual values are homoscedastic

</br>

### B2: Tool Benefits

The programming language of choice for this analysis, as previously mentioned, will be the $\mbox{R}$ programming language. Previously, this author has used Python to perform cleaning, transformation, and analysis. Python has been more than up to the task. However, $\mbox{R}$ happens to handle the process of regression analysis and model selection exceptionally well and thus became the self-evident choice for multiple regression analysis. In particular, the built-in functions of the base $\mbox{R}$ language, as used to fit models, are incredibly simple to navigate and equally as easy to demonstrate. Additionally, the reduction process using the `ols_step_backward_p()` function, as another example, made the task of choosing which programming language to use for this project even easier. It is with good reason that $\mbox{R}$ has such a stellar reputation for handling regression models.

</br>

### B3: Appropriate Technique

Multiple regression is an appropriate technique to use to accomplish our goal of finding which variables contribute to a longer initial length of stay (`Initial_days`) for several reasons. Firstly, the dataset we will analyze contains 50 variables in total, each of disparate significance and utility. While some variable do not require much thought prior to elimination, others are not quite as straightforward. Therefore, running them through a multiple regression model prior to discarding them is an appropriate course of action. Additionally, multiple regression will allow us to see the significance of each variable's discrete contribution to the target variable as well as the interaction between explanatory variables themselves. Overall, regarding our objective, multiple regression is more than up to the task and will adequately suit our objectives.

___
</br>
</br>

## Part III

___

### C1: Data Goals

The process we will need to take to prepare the data for model selection is relatively minor, given that the raw dataset used in this project has already been cleaned in a prior project (see project D206 - Data Cleaning). Using the pre-cleaned dataset, we will first partition the data to include only those variables we intend to feed into our initial model. Because the model selection process we will use is backward-oriented, this initial model will include all features that could possibly have a relationship to the target variable of initial length of hospitalization (`Initial_days`). This will include a mix of numeric and categorical variables. 

Next, we will need to ensure that the data type of each variable is appropriate for that kind of feature. For example, we will determine which categorical variables are nominal and which would benefit from ordinal encoding. Once the dataset for the initial model has been partitioned and transformed (or converted to the right type, at least), we will look over the dataset to ensure that we have not created any problems in the process such as silently introducing null values.

</br>

### C2: Summary Statistics

In order to get the best understanding of the selected features and their measures of central tendency, we will use an amazing library called `skimr` which does a phenomenal job of not only providing a great default summary view of the entire dataframe, but also allows one to customize the output. First, we will fashion a version of the skim function purpose-built for our needs here and print the output.

```{r}
# Set custom skim() for C2: Summary Statistics
# For numeric include mean, median, stdev, min, Q25, Q75, and max
# For factor include count of unique values and value counts for each
my_skim <- skim_with(
  base = sfl(),
  numeric = sfl(Mean = mean,
                Median = median,
                StDev = sd,
                Min = min,
                Q25 = ~ quantile(., probs = .25),
                Q75 = ~ quantile(., probs = .75),
                Max = max),
  factor = sfl(Unique_Values = n_unique,
               Value_Counts = top_counts),
  append = FALSE
)

# Call new skim format
my_skim(init_mdl)
```


The `skim()` output virtually speaks for itself. Our partitioned data for the initial model includes a total of 29 variables comprised of 10 numeric and 19 factor (or categorical) variables. There are a total of 10,000 rows. For the categorical variables, we have shown the names of each variable, total number of unique values for each, and the sum of each unique value for each variable respectively. Additionally, for our 10 numeric type variables, we are provided with each variable's name followed by the mean, median, standard deviation, minimum value, lower quartile (.25), upper quartile (.75), and maximum value respectively. This summary gives us an excellent feel for the selected features for our initial model.

</br>

### C3: Steps to Prepare the Data

Now, we will begin the process of preparing the dataset, starting with loading in the necessary libraries and reading-in the cleaned dataset.

```{r}
# Load in the tidyverse, skimr, and Hmisc
library(tidyverse)
library(skimr)
library(Hmisc)
```

```{r}
# Read dataset in
df <- read.csv("./data/medical_clean.csv")

# Start with a quick skim of the data for orientation and future reference
skim(df)
```

Immediately we will drop variables that, at this time, are unnecessary for our objective and keep the rest.

```{r}
# Partition dataset to include only features to be initially included in model
# Initial_days is reordered to first position for ease of reference
init_mdl <- df %>%
  select(Initial_days,
         Area,
         Children:Services,
         TotalCharge,
         Additional_charges)

# View new dataframe and assess dtypes
init_mdl %>%
  skim()
```

We will next need to convert the data types of many our variables. Since there are not any variables needing type class conversion other than those which are currently of the type character, we will simply select all of those variables at once and convert them to factor variables. This will ensure that the model handles the variables as intended.

```{r}
# Start with reformatting all chr variables as fct
init_mdl[sapply(init_mdl, is.character)] <- lapply(init_mdl[sapply(init_mdl, is.character)], as.factor)

# Reassess dataframe structure using skim()
skim_without_charts(init_mdl)
```

This `skim()` view of the dataframe is quite useful. It shows the dataframes is comprised of two data types: factor and numeric. Thus, it appears we were successful in converting our columns to factors. The majority of the variables included are either dichotomous or otherwise nominal categorical variables. We do, however, have a couple of variables that likely would benefit from ordinal encoding. The previous code was convenient for converting a relatively large group of variables to factors all at once, but does not consider levels. Thus, we will take to re-leveling our ordinal variables (Initial_admin and Complication_risk) below. First, let's just verify our assumptions by accessing these variables' levels and asking whether or not the variable is indeed ordered.

```{r}
# Access levels() for each variable
levels(init_mdl$Initial_admin)
levels(init_mdl$Complication_risk)

# Check if the factor is ordered
is.ordered(init_mdl$Initial_admin)
is.ordered(init_mdl$Complication_risk)
```

OK, our assumptions were valid. Now, let's fix them.

```{r}
# Re-level ordinal categorical variables for Initial_admin
init_mdl$Initial_admin <- factor(init_mdl$Initial_admin,
                                 levels = c("Elective Admission",
                                            "Observation Admission",
                                            "Emergency Admission"),
                                 ordered = TRUE)
# Re-level ordinal categorical variables for Complication_risk
init_mdl$Complication_risk <- factor(init_mdl$Complication_risk,
                                 levels = c("Low",
                                            "Medium",
                                            "High"),
                                 ordered = TRUE)
```

Finally, we will check again to make sure the above worked as intended.

```{r}
# Access levels() for each variable
levels(init_mdl$Initial_admin)
levels(init_mdl$Complication_risk)

# Check if the factor is ordered
is.ordered(init_mdl$Initial_admin)
is.ordered(init_mdl$Complication_risk)
```

Now that we have validated our conversion process and the composition of our data, we can now proceed with the model selection process. 

</br>

### C4: Visualizations

#### Univariate:

Now we will show all of our model's variables using both univariate and bivariate visualizations. We'll start with univariate histograms of the numeric variables, then we'll look at bar charts for all of our categorical variables.

```{r}
# Show histograms for all numeric variables
par(mfrow = c(3,4))
hist(init_mdl %>% 
     select_if(is.numeric))
```

```{r}
# Partition dichotomous Yes/No variables out for plot
dichotomous_vars <- init_mdl %>% 
  select(where(~n_distinct(.) == 2))

# Show bar charts of all Yes/No variables
dichotomous_vars %>%
  gather() %>%
  count(key, value) %>% 
  ggplot(., aes(x = value, y = n)) +
  geom_bar(stat = "identity") +
  facet_wrap(~key, scales = "free", nrow = 3)
```

```{r}
# Partition non-dichotomous categorical variables
cat_vars <- init_mdl %>% 
  select_if(is.factor) %>% 
  select(where(~n_distinct(.) != 2))

# Rename levels to shorter versions to fit plots
levels(cat_vars$Marital) <- c("Divorced",
                              "Married",
                              "Never",
                              "Sep",
                              "Widow")
levels(cat_vars$Initial_admin) <- c("Elective",
                                    "Observation",
                                    "Emergency")

```

```{r}
# Create panel of bar charts for cat_vars
par(mfrow = c(3,2))
barplot(table(cat_vars$Area), main = "Geographical Area")
barplot(table(cat_vars$Marital), main = "Marital Status")
barplot(table(cat_vars$Gender), main = "Gender")
barplot(table(cat_vars$Initial_admin), main = "Reason for Initial Admission")
barplot(table(cat_vars$Complication_risk), main = "Complication Risk")
barplot(table(cat_vars$Services), main = "Services Used")
```

</br>

#### Bivariate:

Now we will get a different view of the data using scatter and box plots to dive a little deeper.

```{r}
# Partition model data for only numeric variables
num_vars <- init_mdl %>% 
  select_if(is.numeric)

# Show scatterplot matrix of numeric variables
pairs(num_vars)
```

```{r}
# Boxplots of our target variable against some categorical variables
par(mfrow = c(2,3))
boxplot(Initial_days ~ Complication_risk, data = init_mdl)
boxplot(Initial_days ~ Gender, data = init_mdl)
boxplot(Initial_days ~ Overweight, data = init_mdl)
boxplot(Initial_days ~ Anxiety, data = init_mdl)
boxplot(Initial_days ~ ReAdmis, data = init_mdl)
boxplot(Initial_days ~ Initial_admin, data = init_mdl)
```

</br>

### C5: Prepared Data Set

For prepared dataset, please see attached .csv file.
___

</br>
</br>

## Part IV

___

### D1: Initial Model



</br>

### D2: Justification of Model Reduction

</br>

### D3: Reduced Multiple Regression Model

___

</br>
</br>

## Part V

___

### E1: Model Comparison

</br>

### E2: Output and Calculations

</br>

### E3: Code

```{r}
init_mdl <- lm(Initial_days ~
               Age +
               Gender +
               ReAdmis +
               VitD_levels +
               Doc_visits +
               Full_meals_eaten +
               vitD_supp +
               Soft_drink +
               Initial_admin +
               HighBlood +
               Stroke +
               Complication_risk +
               Overweight +
               Arthritis +
               Diabetes +
               Hyperlipidemia +
               BackPain +
               Anxiety +
               Allergic_rhinitis +
               Reflux_esophagitis +
               Asthma +
               Services,
               data = df_mdl)

summary(init_mdl)
```

```{r}
df_mdl_emergadmin <- df_mdl %>%
    filter(Initial_admin == "Emergency Admission")
```

```{r}
reduced_mdl <- lm(Initial_days ~
                  VitD_levels +
                  0 +
                  ReAdmis,
                  data = df_mdl_emergadmin)

summary(reduced_mdl)
```

___

</br>
</br>

## Part VI

___

### F1: Results

</br>

### F2: Recommendations

___

</br>
</br>
</br>


### I: Sources

___
